{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "\n",
    "from DatasetCH import UpscaleDataset\n",
    "import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dirs\n",
    "mdir=\"/s2s_nobackup/mpyrina/Downscaling_output/Model_dif/Test_2\"\n",
    "rdir=\"/s2s_nobackup/mpyrina/Downscaling_output/Results_dif/Test_2\"\n",
    "os.makedirs(mdir, exist_ok=True)\n",
    "os.makedirs(rdir, exist_ok=True)\n",
    "\n",
    "# Define the tensorboard writer\n",
    "writer = SummaryWriter(mdir) # was runs_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mpyrina/Notebooks/Diffusion_Downscaling/src_norm')\n",
    "from DatasetCH import *\n",
    "from TrainDiffusion import *\n",
    "#from TrainUnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN DIFFUSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - new upscale\n",
      "Loaded coarse data shape: (460, 11, 16, 32)\n",
      "Loaded high-resolution data shape: (460, 128, 256)\n",
      "Final coarse shape: torch.Size([5060, 1, 16, 32])\n",
      "Final fine shape: torch.Size([5060, 1, 128, 256])\n",
      "Input shape (should be [N, 1, H, W]): torch.Size([5060, 1, 128, 256])\n",
      "Loading static mask...\n",
      "Test - new upscale\n",
      "Loaded coarse data shape: (138, 11, 16, 32)\n",
      "Loaded high-resolution data shape: (138, 128, 256)\n",
      "Final coarse shape: torch.Size([1518, 1, 16, 32])\n",
      "Final fine shape: torch.Size([1518, 1, 128, 256])\n",
      "Input shape (should be [N, 1, H, W]): torch.Size([1518, 1, 128, 256])\n",
      "Loading static mask...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 1\n",
    "accum = 4\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# a tensor of shape [B, C, H, W] mean that c=8, image resol=(H, W) \n",
    "\n",
    "network = Network.EDMPrecond(\n",
    "        img_resolution=(256, 128),\n",
    "        in_channels=2,\n",
    "        out_channels=1,\n",
    "        label_dim=1\n",
    "    ).to(device)\n",
    "\n",
    "# define the datasets\n",
    "ifs_dir = '/s2s/mpyrina/ECMWF_MCH/Europe_eval/s2s_hind_2022/all/'\n",
    "obs_dir = '/net/cfc/s2s_nobackup/mpyrina/TABSD_ifs_like/'\n",
    "mask_dir = '/net/cfc/s2s_nobackup/mpyrina/TABSD_ifs_like/TabsD_mask_static.nc'\n",
    "\n",
    "dataset_train = UpscaleDataset(coarse_data_dir = ifs_dir, highres_data_dir = obs_dir,\n",
    "year_start=2002, year_end=2012, month=815,  \n",
    "constant_variables=None, constant_variables_filename=None, mask_path=mask_dir)\n",
    "\n",
    "dataset_test = UpscaleDataset(coarse_data_dir = ifs_dir, highres_data_dir = obs_dir,\n",
    "year_start=2012, year_end=2015, month=815,  \n",
    "constant_variables=None, constant_variables_filename=None, mask_path=mask_dir)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44084/636993513.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/home/mpyrina/.conda/envs/myenv_iacpy3_2023/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Train :: Epoch: 0:   0%|          | 1/317 [00:00<01:51,  2.82it/s]/home/mpyrina/Notebooks/Diffusion_Downscaling/src_norm/TrainDiffusion.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/mpyrina/.conda/envs/myenv_iacpy3_2023/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1536, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1536, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1024, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 768, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 768, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 512, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 384, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 384, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "THE MASK IS USED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 0:   1%|          | 2/317 [01:10<3:37:53, 41.50s/it, Loss: 1.1231]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1536, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1536, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1024, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 768, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 768, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 512, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 384, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 384, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "THE MASK IS USED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 0:   1%|          | 3/317 [02:18<4:40:28, 53.59s/it, Loss: 1.0688]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1536, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1536, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1024, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 768, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 768, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 512, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 384, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 384, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "THE MASK IS USED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 0:   1%|▏         | 4/317 [03:28<5:14:01, 60.20s/it, Loss: 1.0551]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1536, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1536, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1024, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 768, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 768, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 512, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 384, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 384, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "THE MASK IS USED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 0:   2%|▏         | 5/317 [04:36<5:27:01, 62.89s/it, Loss: 1.0469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 1024, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 2048, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 16, 32]), skips[-1].shape: torch.Size([16, 512, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1536, 16, 32])\n",
      "Input shape to conv0: torch.Size([16, 1024, 16, 32])\n",
      "x.shape: torch.Size([16, 1024, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1536, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 512, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 1024, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 32, 64]), skips[-1].shape: torch.Size([16, 256, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 768, 32, 64])\n",
      "Input shape to conv0: torch.Size([16, 512, 32, 64])\n",
      "x.shape: torch.Size([16, 512, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 768, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 256, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 512, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 64, 128]), skips[-1].shape: torch.Size([16, 128, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 384, 64, 128])\n",
      "Input shape to conv0: torch.Size([16, 256, 64, 128])\n",
      "x.shape: torch.Size([16, 256, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 384, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "x.shape: torch.Size([16, 128, 128, 256]), skips[-1].shape: torch.Size([16, 128, 128, 256])\n",
      "Input shape to conv0: torch.Size([16, 256, 128, 256])\n",
      "THE MASK IS USED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 0:   2%|▏         | 5/317 [05:27<5:40:14, 65.43s/it, Loss: 1.0469]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# best modes\u001b[39;00m\n\u001b[1;32m     15\u001b[0m mbest \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_dif_model_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                               \u001b[49m\u001b[43maccum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Save the model weights\u001b[39;00m\n",
      "File \u001b[0;32m~/Notebooks/Diffusion_Downscaling/src_norm/TrainDiffusion.py:102\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(model, loss_fn, optimiser, data_loader, scaler, step, accum, writer, device)\u001b[0m\n\u001b[1;32m     99\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(loss)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m step_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accum \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/myenv_iacpy3_2023/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv_iacpy3_2023/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv_iacpy3_2023/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m~/.conda/envs/myenv_iacpy3_2023/lib/python3.11/site-packages/torch/autograd/function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "optimiser = torch.optim.AdamW(network.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = EDMLoss()\n",
    "losses = []\n",
    "\n",
    "\n",
    "for step in range(num_epochs):\n",
    "    # model_save\n",
    "    model_save_path = f\"{mdir}/dif_model_epoch_{step}.pt\"\n",
    "    # fig_save\n",
    "    fig_save_path = f\"{rdir}/dif_model_{step}.png\"\n",
    "    # best modes\n",
    "    mbest = f\"{mdir}/best_dif_model_epoch_{step}.pt\"\n",
    "\n",
    "    epoch_loss = training_step(network, loss_fn, optimiser,\n",
    "                                   dataloader_train, scaler, step,\n",
    "                                   accum, writer, device=device)\n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    # Save the model weights\n",
    "    torch.save(network.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    if losses[-1] == min(losses):\n",
    "        torch.save(network.state_dict(), mbest)\n",
    "        \n",
    "    # Plot and save\n",
    "    (fig, ax), (base_error, pred_error), predicted_numpy_array = sample_model_dif(network, dataloader_test, device=device)\n",
    "    plt.show()\n",
    "    fig.savefig(fig_save_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    writer.add_scalar(\"Error/base\", base_error, step)\n",
    "    writer.add_scalar(\"Error/pred\", pred_error, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig, ax), (base_error, pred_error), predicted_numpy_array = sample_model_dif(network, dataloader_test, device=device)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model saved to ./Model_dif/dif_model_epoch_0.pt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = f\"./Model_dif/dif_model_epoch_{step}.pt\"\n",
    "torch.save(network.state_dict(), model_save_path)\n",
    "(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### unet only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the datasets\n",
    "ifs_dir = '/s2s/mpyrina/ECMWF_MCH/Europe_eval/s2s_hind_2022/all/'\n",
    "obs_dir = '/net/cfc/s2s_nobackup/mpyrina/TABSD_ifs_like/'\n",
    "\n",
    "# Run training for small number of epochs \n",
    "num_epochs = 1\n",
    "## Select hyperparameters of training\n",
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "accum = 8\n",
    "\n",
    "dataset_train = UpscaleDataset(coarse_data_dir = ifs_dir, highres_data_dir = obs_dir,\n",
    "year_start=2005, year_end=2008, month=815,  \n",
    "constant_variables=None, constant_variables_filename=None)\n",
    "\n",
    "dataset_test = UpscaleDataset(coarse_data_dir = ifs_dir, highres_data_dir = obs_dir,\n",
    "year_start=2009, year_end=2010, month=815,  \n",
    "constant_variables=None, constant_variables_filename=None)\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Define device\n",
    "device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# define the ml model : 1, 1, : 1 input var, one output\n",
    "unet_model = UNet((256, 128), 1, 1, label_dim=0, use_diffuse=True)\n",
    "unet_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# define the optimiser\n",
    "optimiser = torch.optim.AdamW(unet_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the tensorboard writer\n",
    "writer = SummaryWriter(\"./runs_unet\")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# train the model\n",
    "losses = []\n",
    "for step in range(num_epochs):\n",
    "    epoch_loss = train_step(\n",
    "        unet_model, loss_fn, dataloader_train, optimiser,\n",
    "        scaler, step, accum, writer, device=device)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Save the model weights\n",
    "    model_save_path = f\"./Model/dif_model_epoch_{step}.pt\"\n",
    "    torch.save(unet_model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    (fig, ax), (base_error, pred_error), predicted_numpy_array = sample_model(\n",
    "        unet_model, dataloader_test, device=device)\n",
    "    plt.show()\n",
    "    fig.savefig(f\"./results_unet/{step}.png\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "    writer.add_scalar(\"Error/base\", base_error, step)\n",
    "    writer.add_scalar(\"Error/pred\", pred_error, step)\n",
    "\n",
    "    # save the model\n",
    "    if losses[-1] == min(losses):\n",
    "        torch.save(unet_model.state_dict(), f\"./Model/Models_dif/best_unet_model_epoch_{step}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv_iacpy3_2023]",
   "language": "python",
   "name": "conda-env-.conda-myenv_iacpy3_2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
